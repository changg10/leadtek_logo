# Build a RAG chain for NVIDIA Triton documentation website（为Triton文档网站构建 RAG 工作流）

*In this notebook we demonstrate how to build a RAG using NVIDIA AI Endpoints for LangChain. We create a vector store by downloading web pages and generating their embeddings using FAISS. We then showcase two different chat chains for querying the vector store. For this example, we use the NVIDIA Triton documentation website, though the code can be easily modified to use any other source.（构建 RAG [在本笔记本中，我们演示了如何使用NVIDIA AI Endpoints for LangChain ](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints)。我们通过下载网页并使用 FAISS 生成嵌入来创建矢量存储。然后，我们展示两个不同的聊天链来查询矢量存储。对于此示例，我们使用 NVIDIA Triton 文档网站，但可以轻松修改代码以使用任何其他源*）

## First stage is to load NVIDIA Triton documentation from the web, chunkify the data, and generate embeddings using FAISS（第一阶段是从网络加载NVIDIA Triton文档，将数据分割成块，并使用FAISS生成嵌入向量）

*To run this notebook, you need to complete the [setup](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints#setup) and generate an API key.（要运行此笔记本，您需要完成\[设置]并生成一个API密钥。）*

**这部分代码导入了构建RAG所需的所有库和模块**

```python
import os
from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT
from langchain.chains.question_answering import load_qa_chain

from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
```

*Provide the API key by running the cell below.（通过运行下面的单元格来提供API密钥）*

**这段代码检查是否已经设置了NVIDIA API密钥，如果没有，则提示用户输入。这个密钥用于访问NVIDIA的API**

    import getpass

    if not os.environ.get("NVIDIA_API_KEY", "").startswith("nvapi-"):
        nvapi_key = getpass.getpass("Enter your NVIDIA API key: ")
        assert nvapi_key.startswith("nvapi-"), f"{nvapi_key[:5]}... is not a valid key"
        os.environ["NVIDIA_API_KEY"] = nvapi_key

Helper functions for loading html files, which we'll use to generate the embeddings. We'll use this later to load the relevant html documents from the Triton documentation website and convert to a vector store.（用于加载HTML文件的辅助函数，我们将使用这些函数来生成嵌入向量。稍后，我们将使用这些函数从Triton文档网站加载相关的HTML文档，并转换为矢量存储库。）

**这个函数用于从给定的URL加载HTML文档，并返回其内容。它使用`requests`库来发送HTTP请求，并使用`BeautifulSoup`来解析HTML内容。**

> HTML（HyperText Markup Language，超文本标记语言）是用于创建网页的标准标记语言。
>
> HTML 允许作者通过定义页面的结构和内容来构建网站。它不是一种编程语言，而是一种标记语言，意味着它告诉浏览器如何显示页面上的内容，而不是像编程语言那样提供逻辑处理。

```python
import re    
from typing import List, Union

import requests 
from bs4 import BeautifulSoup 



def html_document_loader(url: Union[str, bytes]) -> str:
    """
    Loads the HTML content of a document from a given URL and return it's content.

    Args:
        url: The URL of the document.

    Returns:
        The content of the document.

    Raises:
        Exception: If there is an error while making the HTTP request.

    """
    try:
        response = requests.get(url)
        html_content = response.text
    except Exception as e:
        print(f"Failed to load {url} due to exception {e}")
        return ""

    try:
        # Create a Beautiful Soup object to parse html
        soup = BeautifulSoup(html_content, "html.parser")

        # Remove script and style tags
        for script in soup(["script", "style"]):
            script.extract()

        # Get the plain text from the HTML document
        text = soup.get_text()

        # Remove excess whitespace and newlines
        text = re.sub("\s+", " ", text).strip()

        return text
    except Exception as e:
        print(f"Exception {e} while loading document")
        return ""
```

Read html files and split text in preparation for embedding generation Note chunk\_size value must match the specific LLM used for embedding genetation（阅读HTML文件并分割文本，为生成嵌入（embedding）做准备。请注意，chunk\_size的值必须与用于嵌入生成的具体大型语言模型（LLM）相匹配。）

Make sure to pay attention to the chunk\_size parameter in TextSplitter. Setting the right chunk size is critical for RAG performance, as much of a RAG’s success is based on the retrieval step finding the right context for generation. The entire prompt (retrieved chunks + user query) must fit within the LLM’s context window. Therefore, you should not specify chunk sizes too big, and balance them out with the estimated query size. For example, while OpenAI LLMs have a context window of 8k-32k tokens, Llama3 is limited to 8k tokens. Experiment with different chunk sizes, but typical values should be 100-600, depending on the LLM.（务必注意TextSplitter中的chunk\_size参数。设置正确的块大小对于RAG（检索增强生成）性能至关重要，因为RAG的成功很大程度上取决于检索步骤找到用于生成的正确上下文的能力。整个提示（检索到的块+用户查询）必须适应LLM的上下文窗口。因此，你不应指定过大的块大小，并应根据估计的查询大小来平衡它们。例如，虽然OpenAI的LLM有8k-32k令牌的上下文窗口，Llama3则限制为8k令牌。尝试使用不同的块大小，但典型的值应在100-600之间，具体取决于LLM。）

**这个函数用于创建嵌入。它首先定义了一个包含NVIDIA Triton技术文档网页的URL列表，然后使用`html_document_loader`函数加载这些网页的内容，并使用`RecursiveCharacterTextSplitter`将文本分割成块，最后调用`index_docs`函数来生成嵌入并将它们存储到指定的目录。**

```python
def create_embeddings(embedding_path: str = "./data/nv_embedding"):

    embedding_path = "./data/nv_embedding"
    print(f"Storing embeddings to {embedding_path}")

    # List of web pages containing NVIDIA Triton technical documentation
    urls = [
         "https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html",
         "https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html",
         "https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html",
         "https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_analyzer.html",
         "https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html",
    ]

    documents = []
    for url in urls:
        document = html_document_loader(url)
        documents.append(document)


    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=0,
        length_function=len,
    )
    texts = text_splitter.create_documents(documents)
    index_docs(url, text_splitter, texts, embedding_path)
    print("Generated embedding successfully")
```

Generate embeddings using NVIDIA AI Endpoints for LangChain and save embeddings to offline vector store in the ./data/nv\_embedding directory for future re-use（使用NVIDIA AI Endpoints为LangChain生成嵌入（embeddings），并将这些嵌入保存到本地的向量存储中，路径为./data/nv\_embedding目录，以便将来重复使用）

**这个函数用于将文档分割成块，并为每个块创建嵌入，然后将这些嵌入存储到向量存储中。**

    def index_docs(url: Union[str, bytes], splitter, documents: List[str], dest_embed_dir) -> None:
        """
        Split the document into chunks and create embeddings for the document

        Args:
            url: Source url for the document.
            splitter: Splitter used to split the document
            documents: list of documents whose embeddings needs to be created
            dest_embed_dir: destination directory for embeddings

        Returns:
            None
        """
        embeddings = NVIDIAEmbeddings(model="NV-Embed-QA", truncate="END")

        for document in documents:
            texts = splitter.split_text(document.page_content)

            # metadata to attach to document
            metadatas = [document.metadata]

            # create embeddings and add to vector store
            if os.path.exists(dest_embed_dir):
                update = FAISS.load_local(folder_path=dest_embed_dir, embeddings=embeddings, allow_dangerous_deserialization=True)
                update.add_texts(texts, metadatas=metadatas)
                update.save_local(folder_path=dest_embed_dir)
            else:
                docsearch = FAISS.from_texts(texts, embedding=embeddings, metadatas=metadatas)
                docsearch.save_local(folder_path=dest_embed_dir)

**调用`create_embeddings`函数来生成嵌入**

    create_embeddings()

## Second stage is to load the embeddings from the vector store and build a RAG using NVIDIAEmbeddings（第二阶段：从向量存储中加载嵌入，并使用NVIDIAEmbeddings构建RAG）

Create the embeddings model using NVIDIA Retrieval QA Embedding endpoint. This model represents words, phrases, or other entities as vectors of numbers and understands the relation between words and phrases. See here for reference: <https://build.nvidia.com/nvidia/embed-qa-4>（使用NVIDIA检索问答嵌入端点（Retrieval QA Embedding endpoint）创建嵌入模型。该模型将单词、短语或其他实体表示为数字向量，并理解单词和短语之间的关系。有关参考，请参见：[NVIDIA嵌入QA 4](https://build.nvidia.com/nvidia/embed-qa-4)。）

**创建了一个使用NVIDIA检索QA嵌入端点的嵌入模型**

```python
embedding_model = NVIDIAEmbeddings(model="NV-Embed-QA", truncate="END", allow_dangerous_deserialization=True)
```

Load documents from vector database using FAISS（使用FAISS从向量数据库加载文档）

> FAISS（Facebook AI Similarity Search）是由Facebook AI Research开发的一个高效相似性搜索库，特别适用于大规模向量搜索任务。它能够快速地在大量的高维向量中找到最接近给定查询向量的那些向量。FAISS支持多种索引结构，可以用于精确搜索或近似搜索，并且能够在GPU上加速搜索过程。

**加载了之前保存的嵌入，并创建了一个检索器，用于从向量存储中检索相关文档。**

```python
# Embed documents
embedding_path = "./data/nv_embedding"
docsearch = FAISS.load_local(folder_path=embedding_path, embeddings=embedding_model, allow_dangerous_deserialization=True)
retriever = docsearch.as_retriever()
```

    # This should return documents related to the test query
    retriever.invoke("Deploy TensorRT-LLM Engine on Triton Inference Server")

*Create a ConversationalRetrievalChain chain. In this chain we demonstrate the use of 2 LLMs: one for summarization and another for chat. This improves the overall result in more complicated scenarios. We'll use Llama3 70B for the first LLM and Mixtral for the Chat element in the chain. We add a question\_generator to generate relevant query prompt. See here for reference: <https://python.langchain.com/docs/modules/chains/popular/chat_vector_db#conversationalretrievalchain-with-streaming-to-stdout>*

**打印出用于问题生成和问答的提示模板**

```python
print(f"{CONDENSE_QUESTION_PROMPT = }")
print(f"{QA_PROMPT = }")
```

**创建了一个问答链，它使用两个LLM：一个用于问题生成，另一个用于聊天。这个链首先使用`CONDENSE_QUESTION_PROMPT`提示模板来生成问题，然后使用`retriever`来检索相关文档，最后使用`QA_PROMPT`提示模板来生成答案。**

```python
llm = ChatNVIDIA(model='mistralai/mixtral-8x7b-instruct-v0.1')
chat = ChatNVIDIA(model="mistralai/mixtral-8x7b-instruct-v0.1", temperature=0.1, max_tokens=1000, top_p=1.0)

retriever = docsearch.as_retriever()

## Requires question and chat_history
qa_chain = (RunnablePassthrough()
    ## {question, chat_history} -> str
    | CONDENSE_QUESTION_PROMPT | llm | StrOutputParser()
    # | RunnablePassthrough(print)
    ## str -> {question, context}
    | {"question": lambda x: x, "context": retriever}
    # | RunnablePassthrough(print)
    ## {question, context} -> str
    | QA_PROMPT | chat | StrOutputParser()
)
```

*Ask any question about Triton*

**询问关于Triton的问题，并使用问答链来生成答案**

```python
chat_history = []

query = "What is Triton?"
chat_history += [qa_chain.invoke({"question": query, "chat_history": chat_history})]
chat_history
```

Ask another question about Triton

**询问另一个关于Triton的问题，并使用问答链来生成答案**

    query = "What interfaces does Triton support?"
    chat_history += [""]
    for token in qa_chain.stream({"question": query, "chat_history": chat_history[:-1]}):
        print(token, end="")
        chat_history[-1] += token

*Finally showcase chat capabilites by asking a question about the previous query*

**询问关于之前问题的问题，并使用问答链来生成答案**

```python
query = "But why?"
for token in qa_chain.stream({"question": query, "chat_history": chat_history}):
    print(token, end="")
```

Now we demonstrate a simpler chain using a single LLM only, a chat LLM

    chat = ChatNVIDIA(
        model='mistralai/mixtral-8x7b-instruct-v0.1', 
        temperature=0.1, 
        max_tokens=1000, 
        top_p=1.0
    )

    qa_prompt = ChatPromptTemplate.from_messages([
        ("user", 
            "Use the following pieces of context to answer the question at the end."
            " If you don't know the answer, just say that you don't know, don't try to make up an answer."
            "\n\nHISTORY: {history}\n\n{context}\n\nQuestion: {question}\nHelpful Answer:"
        )
    ])

    ## Requires question and chat_history
    qa_chain = (
        RunnablePassthrough.assign(context = (lambda state: state.get("question")) | retriever)
        # | RunnablePassthrough(print)
        | qa_prompt | chat | StrOutputParser()
    )

Now try asking a question about Triton with the simpler chain. Compare the answer to the result with previous complex chain model

    chat_history = []

    query = "What is Triton?"
    chat_history += [qa_chain.invoke({"question": query, "history": chat_history})]
    chat_history

Ask another question about Triton

    query = "Does Triton support ONNX?"
    chat_history += [""]
    for token in qa_chain.stream({"question": query, "history": chat_history[:-1]}):
        print(token, end="")
        chat_history[-1] += token

Finally showcase chat capabilites by asking a question about the previous query

    query = "How come?"
    for token in qa_chain.stream({"question": query, "history": chat_history}):
        print(token, end="")

